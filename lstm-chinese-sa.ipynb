{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 实现中文文本情感分析\n",
    "给出数据集路径，划分训练集测试集，完成训练，输出准确率，并保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "import gensim\n",
    "import time\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_path = 'data/neg.txt'\n",
    "pos_path = 'data/pos.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/n6/fr8x2twx7v9b5c7qqz1d_d640000gn/T/jieba.cache\n",
      "Loading model cost 0.711 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/var/folders/n6/fr8x2twx7v9b5c7qqz1d_d640000gn/T/ipykernel_17789/4161536856.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(neg_data+pos_data)\n"
     ]
    }
   ],
   "source": [
    "# 分词器\n",
    "def tokenizer(text):\n",
    "    result = jieba.lcut(text)\n",
    "    return result\n",
    "\n",
    "# 预处理数据\n",
    "# 读取数据\n",
    "neg_data = open(neg_path, 'r', encoding='utf-8').read().split('\\n')\n",
    "pos_data = open(pos_path, 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "# 分词\n",
    "neg_data = [tokenizer(x) for x in neg_data]\n",
    "pos_data = [tokenizer(x) for x in pos_data]\n",
    "\n",
    "# 划分训练集、测试集\n",
    "data = np.array(neg_data+pos_data)\n",
    "label = np.concatenate((np.zeros(len(neg_data)), np.ones(len(pos_data))))\n",
    "train_X, test_X, train_y, test_y = train_test_split(data, label, test_size=0.2, random_state=5)\n",
    "\n",
    "# 构建词表\n",
    "vocab = set(chain(*data))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=500, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(pad_samples(encode_samples(train_X, vocab)))\n",
    "train_labels = torch.tensor([score for score in train_y])\n",
    "test_features = torch.tensor(pad_samples(encode_samples(test_X, vocab)))\n",
    "test_labels = torch.tensor([score for score in test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52240, 37623,     0,  ...,     0,     0,     0],\n",
       "        [14502, 54732,     0,  ...,     0,     0,     0],\n",
       "        [49312, 32686, 70467,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [41968, 26860,  2939,  ...,     0,     0,     0],\n",
       "        [14427, 26860, 55932,  ...,     0,     0,     0],\n",
       "        [61185, 66795, 50773,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tencent 预训练的词向量文件路径\n",
    "vec_path = \"/share_v3/fangcheng/data/Tencent_AILab_ChineseEmbedding.txt\"\n",
    "# 加载词向量文件\n",
    "wvmodelwvmodel = gensim.models.KeyedVectors.load_word2vec_format(vec_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, labels, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "embed_size = 100\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.8\n",
    "device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = SentimentNet(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional,\n",
    "                   labels=labels)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_losses = 0, 0\n",
    "    train_acc, test_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "        net.zero_grad()\n",
    "        # feature = Variable(feature.cuda())\n",
    "        # label = Variable(label.cuda())\n",
    "        score = net(feature)\n",
    "        # print(score.type)\n",
    "        # score = score.to(device=device, dtype=torch.long)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_label in test_iter:\n",
    "            m += 1\n",
    "            # test_feature = test_feature.cuda()\n",
    "            # test_label = test_label.cuda()\n",
    "            test_score = net(test_feature)\n",
    "            test_loss = loss_function(test_score, test_label)\n",
    "            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                    dim=1), test_label.cpu())\n",
    "            test_losses += test_loss\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
    "          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbfce0d0ecd88c9869dee4e2dd4adee30747199cef57445604a0f2a5dd0c4273"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
